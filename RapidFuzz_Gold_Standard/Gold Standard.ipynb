{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f9c9345-6a4c-40ef-811e-2f9743da144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.10.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Downloading rapidfuzz-3.10.1-cp312-cp312-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f5b2e63f-e9df-4825-a257-b0ff33b4d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2024.10.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting click>=8.1 (from dask)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpickle>=3.0.0 (from dask)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /opt/miniconda3/envs/ML_Course/lib/python3.12/site-packages (from dask) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/ML_Course/lib/python3.12/site-packages (from dask) (24.1)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/miniconda3/envs/ML_Course/lib/python3.12/site-packages (from dask) (6.0.1)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting locket (from partd>=1.4.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading dask-2024.10.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: toolz, locket, cloudpickle, click, partd, dask\n",
      "Successfully installed click-8.1.7 cloudpickle-3.1.0 dask-2024.10.0 locket-1.0.0 partd-1.4.2 toolz-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "724ed136-5c26-4281-8a9a-53548c629c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import dask.dataframe as dd\n",
    "from rapidfuzz import process, fuzz\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3cfc9bb2-93bb-4d7b-8c8b-00c8ad39142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.5"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio(\"this is a test\", \"this is a new test!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2bfc7b73-c12b-4b89-b64d-2ccca1abe697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_set_ratio(\"this is a test\", \"this is a new test!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c733f98-03c0-433e-a31a-2243c3b8d044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.88888888888889"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio(\"Love Song Taylor A\", \"Love Song (Taylor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22584636-caf0-4e37-8068-4d8b927537b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.48648648648648"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_set_ratio(\"Love Song Taylor AA\", \"Love Song (Taylor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a219c981-09bd-4e5b-8402-d50005b949fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.5"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.WRatio(\"Out of This World\", \"Always Running Out of Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a3abf8a9-ac01-4892-8bcd-7583b1d44d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rapidfuzz.distance import Levenshtein\n",
    "Levenshtein.normalized_similarity(\"Out of This World\", \"Always Running Out of Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "56afe841-3036-43dc-8249-81ea410d6f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18181818181818177"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rapidfuzz.distance import Levenshtein\n",
    "Levenshtein.normalized_similarity(\"Circus Music\", \"Exit Music (For A Dub)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222114f-492f-49a1-9508-afd63ed74b9d",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5f0b83e-d77c-4a69-9130-e9a3636d6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load only necessary columns\n",
    "# df_opendb = pd.read_csv('/Users/anhnhat/Library/Mobile Documents/com~apple~CloudDocs/Documents/UNIMA/2. Semester Study/2. WS2425/4. W24 Web Data Integration/Project/Reduce Dataset/opendb_s.csv', usecols=['track_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d38093da-bcde-429d-a6cb-24c13157584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_opendb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f46eda69-49e0-4591-8e0f-ae1f440d51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load only necessary columns\n",
    "# df_am = pd.read_csv('/Users/anhnhat/Library/Mobile Documents/com~apple~CloudDocs/Documents/UNIMA/2. Semester Study/2. WS2425/4. W24 Web Data Integration/Project/Reduce Dataset/apple_music_dataset.csv',usecols=['trackCensoredName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f1f2f4a-92b1-43ae-8111-944fdb9d146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load only necessary columns\n",
    "# df_am = pd.read_csv('/Users/anhnhat/Library/Mobile Documents/com~apple~CloudDocs/Documents/UNIMA/2. Semester Study/2. WS2425/4. W24 Web Data Integration/Project/Reduce Dataset/apple_music_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "973c15df-39d8-488c-9b43-b76b303432b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_am.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4ad2fd7-1ccc-49dd-99e7-43b9e6249779",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/anhnhat/Library/Mobile Documents/com~apple~CloudDocs/Documents/UNIMA/2. Semester Study/2. WS2425/4. W24 Web Data Integration/Project/Reduce Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a56267f3-4c38-4bfc-98ec-0a6da253b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                 Track\n",
      "0  A1                                 Music\n",
      "1  A2                                 Music\n",
      "2  A3  Don't Stop Believin' (2024 Remaster)\n",
      "3  A4                             I'm Yours\n",
      "4  A5                                 Music\n"
     ]
    }
   ],
   "source": [
    "# Apple music\n",
    "ids, tracks = [], []\n",
    "\n",
    "# Parse XML file iteratively\n",
    "for event, elem in ET.iterparse(path + 'apple.xml', events=('end',)):\n",
    "    if elem.tag == 'song':  # Look for each <song> element\n",
    "        # Extract ID and Track fields\n",
    "        id_value = elem.find('id').text if elem.find('id') is not None else None\n",
    "        track_value = elem.find('Track').text if elem.find('Track') is not None else None\n",
    "        \n",
    "        # Append extracted data to lists\n",
    "        ids.append(id_value)\n",
    "        tracks.append(track_value)\n",
    "\n",
    "        # Clear element to free memory\n",
    "        elem.clear()\n",
    "\n",
    "# Create DataFrame with the extracted columns\n",
    "df_am = pd.DataFrame({'ID': ids, 'Track': tracks})\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(df_am.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f93f8520-2cec-4dc4-9adb-2955fb59adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_and_track(xml_file):\n",
    "    # Initialize lists to store the extracted data\n",
    "    ids, tracks = [], []\n",
    "\n",
    "    # Parse the XML file iteratively\n",
    "    for event, elem in ET.iterparse(path + xml_file, events=('end',)):\n",
    "        if elem.tag == 'song':  # Look for each <song> element\n",
    "            # Extract ID and Track fields\n",
    "            id_value = elem.find('id').text if elem.find('id') is not None else None\n",
    "            track_value = elem.find('Track').text if elem.find('Track') is not None else None\n",
    "            \n",
    "            # Append extracted data to lists\n",
    "            ids.append(id_value)\n",
    "            tracks.append(track_value)\n",
    "\n",
    "            # Clear element to free memory\n",
    "            elem.clear()\n",
    "\n",
    "    # Create DataFrame with the extracted columns\n",
    "    df = pd.DataFrame({'ID': ids, 'Track': tracks})\n",
    "\n",
    "    # Display the DataFrame to verify\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e866d30a-3f3a-4e89-9546-3d7d02cf6265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                 Track\n",
      "0  A1                                 Music\n",
      "1  A2                                 Music\n",
      "2  A3  Don't Stop Believin' (2024 Remaster)\n",
      "3  A4                             I'm Yours\n",
      "4  A5                                 Music\n"
     ]
    }
   ],
   "source": [
    "# Apple Music\n",
    "df_am = extract_id_and_track('apple.xml')\n",
    "print(df_am.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e6214762-ded7-4d0a-a7e4-4ffa3de5b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID            Track\n",
      "0  C1   Mr. Brightside\n",
      "1  C2       Wonderwall\n",
      "2  C3  Come as You Are\n",
      "3  C4      Take Me Out\n",
      "4  C5            Creep\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df_million = extract_id_and_track('million.xml')\n",
    "print(df_million.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "de744c8b-67dc-44b0-b6d9-21797e0536aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                          Track\n",
      "0  B1                         Replay\n",
      "1  B2   Replay (Made Famous by Iyaz)\n",
      "2  B3   Replay It (feat. DA Wallach)\n",
      "3  B4                    Replay This\n",
      "4  B5  Replay This (Sped-Up Version)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df_opendb= extract_id_and_track('opendb.xml')\n",
    "print(df_opendb.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e6737904-fa2e-4fe0-8f60-251455e47a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows of dataset Apple Music: 10000\n",
      "Number rows of dataset Millions: 50683\n",
      "Number rows of dataset Open DB: 1352074\n"
     ]
    }
   ],
   "source": [
    "print('Number rows of dataset Apple Music: ' + str(df_am.shape[0]))\n",
    "print('Number rows of dataset Millions: ' + str(df_million.shape[0]))\n",
    "print('Number rows of dataset Open DB: ' + str(df_opendb.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "838d7d4a-cdbe-472b-8f5e-9c1cfd5d9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opendb_100k = df_opendb.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b475d32c-e2b2-4c80-8c22-b668bea05eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows of dataset Open DB random 100k: 100000\n"
     ]
    }
   ],
   "source": [
    "print('Number rows of dataset Open DB random 100k: ' + str(df_opendb_100k.shape[0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e8ddc5d6-4a6f-49bb-90dc-3b5ce1ffce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_am = df_am.sample(n=2000)\n",
    "# df_million = df_million.sample(n=2000)\n",
    "# df_opendb_100k = df_opendb_100k.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ae98ad70-278d-46ae-83b6-c80ecdf6c0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-10-30 23:55:49'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5a465146-de64-469d-ba81-cd1e827f4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_large_with_small(df_large, df_small, large_track_col='Track', small_track_col='Track', npartitions=10):\n",
    "    \"\"\"\n",
    "    Compare a large Dask DataFrame with a smaller Pandas DataFrame using RapidFuzz similarity metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        df_large (pd.DataFrame or dd.DataFrame): Large Dask DataFrame to compare.\n",
    "        df_small (pd.DataFrame): Smaller Pandas DataFrame for comparison.\n",
    "        large_track_col (str): Column name in df_large containing track names.\n",
    "        small_track_col (str): Column name in df_small containing track names.\n",
    "        npartitions (int): Number of partitions for Dask DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        dd.DataFrame: Dask DataFrame with similarity scores and matched track from df_small.\n",
    "    \"\"\"\n",
    "    # Convert large DataFrame to Dask if it's not already\n",
    "    if not isinstance(df_large, dd.DataFrame):\n",
    "        df_large = dd.from_pandas(df_large, npartitions=npartitions)\n",
    "\n",
    "    # Define the function to get the similarity score and matched track\n",
    "    def get_match_details(track):\n",
    "        best_match = process.extractOne(track, df_small[small_track_col], scorer=fuzz.WRatio)\n",
    "        score, matched_track = best_match[1], best_match[0] if best_match else (None, None)\n",
    "        return score, matched_track\n",
    "\n",
    "    # Apply the function and separate the results into two columns\n",
    "    df_large['match_details'] = df_large[large_track_col].apply(get_match_details, meta=(large_track_col, 'object'))\n",
    "    df_large = df_large.assign(\n",
    "        similarity_score=df_large['match_details'].apply(lambda x: x[0], meta=('x', 'float')),\n",
    "        matched_track=df_large['match_details'].apply(lambda x: x[1], meta=('x', 'object'))\n",
    "    )\n",
    "\n",
    "    # Drop the intermediate tuple column\n",
    "    df_large = df_large.drop(columns='match_details')\n",
    "\n",
    "    return df_large\n",
    "\n",
    "# Example usage\n",
    "# Compare large dataset with df_am\n",
    "df_result_am = compare_large_with_small(df_opendb_100k, df_am)\n",
    "df_result_am = df_result_am.compute()  # Convert back to Pandas if needed\n",
    "\n",
    "# Compare large dataset with million\n",
    "df_result_million = compare_large_with_small(df_opendb_100k,df_million )\n",
    "df_result_million = df_result_million.compute()  # Convert back to Pandas if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b6430e6e-5f8f-45ba-a431-47743365e84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Track</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>matched_track</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>B67</td>\n",
       "      <td>Replay (O meu time é a alegria da cidade)</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Replay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>B99</td>\n",
       "      <td>Replay</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Replay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>B288</td>\n",
       "      <td>Replay</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Replay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>B412</td>\n",
       "      <td>Replay (Sped Up Version) (feat. Ashley Jana)</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Replay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>B607</td>\n",
       "      <td>Circus Music</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                         Track  similarity_score  \\\n",
       "66    B67     Replay (O meu time é a alegria da cidade)              90.0   \n",
       "98    B99                                        Replay             100.0   \n",
       "287  B288                                        Replay             100.0   \n",
       "411  B412  Replay (Sped Up Version) (feat. Ashley Jana)              90.0   \n",
       "606  B607                                  Circus Music              90.0   \n",
       "\n",
       "    matched_track  \n",
       "66         Replay  \n",
       "98         Replay  \n",
       "287        Replay  \n",
       "411        Replay  \n",
       "606         Music  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_am.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9fae4c82-55b2-4cff-b8eb-3cea12546070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Track</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>matched_track</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>B67</td>\n",
       "      <td>Replay (O meu time é a alegria da cidade)</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>Ghost of a Chance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>B99</td>\n",
       "      <td>Replay</td>\n",
       "      <td>65.454545</td>\n",
       "      <td>Ready 2 Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>B288</td>\n",
       "      <td>Replay</td>\n",
       "      <td>65.454545</td>\n",
       "      <td>Ready 2 Wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>B412</td>\n",
       "      <td>Replay (Sped Up Version) (feat. Ashley Jana)</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>Further On Up The Road</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>B607</td>\n",
       "      <td>Circus Music</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>Exit Music (For A Dub)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                                         Track  similarity_score  \\\n",
       "66    B67     Replay (O meu time é a alegria da cidade)         85.500000   \n",
       "98    B99                                        Replay         65.454545   \n",
       "287  B288                                        Replay         65.454545   \n",
       "411  B412  Replay (Sped Up Version) (feat. Ashley Jana)         85.500000   \n",
       "606  B607                                  Circus Music         85.500000   \n",
       "\n",
       "              matched_track  \n",
       "66        Ghost of a Chance  \n",
       "98             Ready 2 Wear  \n",
       "287            Ready 2 Wear  \n",
       "411  Further On Up The Road  \n",
       "606  Exit Music (For A Dub)  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_million.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "467c15bd-e152-4a46-89bb-3c56dbdb4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the results are in Pandas format\n",
    "df_result_am = df_result_am.compute() if isinstance(df_result_am, dd.DataFrame) else df_result_am\n",
    "df_result_million = df_result_million.compute() if isinstance(df_result_million, dd.DataFrame) else df_result_million\n",
    "\n",
    "# Write each DataFrame to CSV\n",
    "df_result_am.to_csv('df_result_am.csv', index=False)\n",
    "df_result_million.to_csv('df_result_million.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c920aefd-27da-4745-83b7-888e879eef54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-10-31 00:20:33'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4011d768-f1b5-4d34-a6f9-7ec93facb9ed",
   "metadata": {},
   "source": [
    "### normalized Levenshtein similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dede990b-ac18-4370-bf1a-8db1bbf92381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2024-10-31 00:28:11.126913\n",
      "End Time: 2024-10-31 00:30:26.194113\n",
      "Runtime: 0:02:15.067200\n",
      "Results saved to df_result_am.csv\n",
      "Start Time: 2024-10-31 00:30:26.352973\n",
      "End Time: 2024-10-31 00:40:41.215179\n",
      "Runtime: 0:10:14.862206\n",
      "Results saved to df_result_million.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from rapidfuzz import process\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "from datetime import datetime\n",
    "\n",
    "def compare_large_with_small(df_large, df_small, output_file, large_track_col='Track', small_track_col='Track', npartitions=10, score_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Compare a large Dask DataFrame with a smaller Pandas DataFrame using Levenshtein normalized similarity with a score threshold,\n",
    "    track runtime, and save results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        df_large (pd.DataFrame or dd.DataFrame): Large Dask DataFrame to compare.\n",
    "        df_small (pd.DataFrame): Smaller Pandas DataFrame for comparison.\n",
    "        output_file (str): Name of the output CSV file.\n",
    "        large_track_col (str): Column name in df_large containing track names.\n",
    "        small_track_col (str): Column name in df_small containing track names.\n",
    "        npartitions (int): Number of partitions for Dask DataFrame.\n",
    "        score_threshold (float): Minimum similarity score to consider a match, between 0 and 1.\n",
    "        \n",
    "    Returns:\n",
    "        dd.DataFrame: Dask DataFrame with similarity scores and matched track from df_small.\n",
    "    \"\"\"\n",
    "    # Record start time\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Start Time: {start_time}\")\n",
    "\n",
    "    # Convert large DataFrame to Dask if it's not already\n",
    "    if not isinstance(df_large, dd.DataFrame):\n",
    "        df_large = dd.from_pandas(df_large, npartitions=npartitions)\n",
    "\n",
    "    # Define the function to get the normalized Levenshtein similarity score and matched track\n",
    "    def get_match_details(track):\n",
    "        best_match = process.extractOne(\n",
    "            track,\n",
    "            df_small[small_track_col],\n",
    "            scorer=Levenshtein.normalized_similarity,\n",
    "            score_cutoff=score_threshold  # Apply the score threshold\n",
    "        )\n",
    "        if best_match:\n",
    "            score, matched_track = best_match[1], best_match[0]\n",
    "        else:\n",
    "            score, matched_track = None, None\n",
    "        return score, matched_track\n",
    "\n",
    "    # Apply the function and separate the results into two columns\n",
    "    df_large['match_details'] = df_large[large_track_col].apply(get_match_details, meta=(large_track_col, 'object'))\n",
    "    df_large = df_large.assign(\n",
    "        similarity_score=df_large['match_details'].apply(lambda x: x[0], meta=('x', 'float')),\n",
    "        matched_track=df_large['match_details'].apply(lambda x: x[1], meta=('x', 'object'))\n",
    "    )\n",
    "\n",
    "    # Drop the intermediate tuple column\n",
    "    df_large = df_large.drop(columns='match_details')\n",
    "\n",
    "    # Compute the Dask DataFrame to get the result in Pandas format\n",
    "    df_result = df_large.compute()\n",
    "\n",
    "    # Record end time\n",
    "    end_time = datetime.now()\n",
    "    print(f\"End Time: {end_time}\")\n",
    "\n",
    "    # Calculate and print runtime\n",
    "    runtime = end_time - start_time\n",
    "    print(f\"Runtime: {runtime}\")\n",
    "\n",
    "    # Write the result to CSV\n",
    "    df_result.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "    return df_result\n",
    "\n",
    "# Example usage\n",
    "# Compare large dataset with df_am and save results to CSV with score threshold of 0.6\n",
    "df_result_am = compare_large_with_small(df_opendb_100k, df_am, 'df_result_am.csv', score_threshold=0.6)\n",
    "\n",
    "# Compare large dataset with df_million and save results to CSV with score threshold of 0.6\n",
    "df_result_million = compare_large_with_small(df_opendb_100k, df_million, 'df_result_million.csv', score_threshold=0.6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
