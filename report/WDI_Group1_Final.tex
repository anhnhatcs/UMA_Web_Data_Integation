% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
\usepackage{cite}
\usepackage{booktabs}
\usepackage[colorlinks, linkcolor=blue]{hyperref}
\usepackage[table]{xcolor}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{threeparttable}
\usepackage{array} % For table enhancements
\usepackage[most]{tcolorbox}

\setcounter{secnumdepth}{3}
\makeatletter
\def\UrlAlphabet{%
	\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j%
	\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t%
	\do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D%
	\do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N%
	\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X%
	\do\Y\do\Z}
\def\UrlDigits{\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\0}
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\g@addto@macro{\UrlBreaks}{\UrlAlphabet}
\g@addto@macro{\UrlBreaks}{\UrlDigits}
\makeatother
%
\begin{document}
	%
	\pagestyle{plain} % or \pagestyle{headings} for headers
	
	\title{Web Data Integration\\Open Music Data Integration}
	%
	%\titlerunning{Abbreviated paper title}
	% If the paper title is too long for the running head, you can set
	% an abbreviated paper title here
	%
	\author{Anh-Nhat Nguyen\orcidID{2034311} \and
		Ching-Yun Cheng\orcidID{2112322}\and
		Shamalan Rajesvaran\orcidID{2115475} \and 
		Yen-An Chen\orcidID{2113612} \and 
		Phelan Lee Yeuk Bun\orcidID{2053019} }
	%
	\authorrunning{Anh-Nhat Nguyen et al.}
	% First names are abbreviated in the running head.
	% If there are more than two authors, 'et al.' is used.
	%
	\institute{University of Mannheim, Germany - Team 1} 
	%\and
	%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
	%\email{lncs@springer.com}\\
	%\url{http://www.springer.com/gp/computer-science/lncs} \and
	%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
	%\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\section{Introduction}
In this Web Data Integration project, we aim to consolidate and analyze standardization from multiple sources to gain comprehensive insights into music streaming trends, track performance, and audience preferences. The project leverages three key datasets:

1. Million Song Dataset with Spotify and Last.fm Features Dataset (csv) \cite{Million_Spot_LastFM}: This dataset is an enriched version of the Million Song Dataset, a large-scale music database containing detailed metadata and audio features for over 50,000 tracks with 21 attributes. It integrates additional attributes from Spotify and Last.fm, including audio features like danceability, energy, loudness, and popularity metrics such as tags (list attribute), preview URLs, and genre classifications. The merging of these three data sources provides a comprehensive view of each song, making it suitable for analyzing music trends, listener behaviors, and track popularity across platforms.

2. Apple Music Tracks (csv) \cite{Apple_Music}: This dataset contains detailed information on 10,000 tracks with 24 attributes sourced from Apple Music. It includes attributes such as artist names, album titles, track features (e.g., tempo, key, mode), and genre classifications. In addition to audio metadata, the dataset provides insights into song popularity metrics and trends across the platform. It is ideal for exploring the characteristics of songs on Apple Music, understanding artist performance, and analyzing trends in genres and musical features.

3. Openmusic API Dataset (json) \cite{open_music}: This dataset offers details about over 5,500 tracks via web APIs (retrieved by /explore \& /album?id=<AlbumID>), including track and album metadata, artist information, and playback types (clean/explicit). The use case for this dataset revolves around leveraging real-time API data for in-depth analysis of track consumption patterns, artist popularity, and changes in audience preferences over time.

Together, these datasets will help provide a holistic view of how various musical, commercial, and audience factors contribute to the success of music tracks on different platforms based on various algorithm approach \cite{doan_principles_2012}.

\section{Data Collection and Data Translation}
\subsection{Data Collection and Dataset}
The dataset was obtained from Kaggle in the form of $csv$ and OpenMusic API in the form of $csv$ and $json$. An overview of dataset attributes is presented in Table~\ref{tab1} below.

\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\caption{Dataset structure}\label{tab1}
	\centering
	\begin{tabular}{p{3cm} p{3cm}p{3cm}p{5cm}}
		\toprule
		\textbf{Dataset} &  \textbf{ No Entities}&\textbf{No. Attributes}& \textbf{Attributes}\\
		\hline
		\hline
		Million Song Dataset with Spotify and Last.fm Features&50,683&21&Track ID, Name, Artist, Spotify Preview URL, Spotify ID, Tags, Genre (\emph{MV 56\%}), Year, Duration MS, Danceability, Energy, Key, Loudness, Mode, Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo, Time Signature\\
		Apple Music Tracks&10,000&24
		&Artist ID, Artist Name, Collection Censored Name, Collection ID, Collection Name, Collection Price, Content Advisory Rating (\emph{MV 85\%}), Country, Currency, Disc Count, Disc Number, Is Streamable, Kind, Preview URL, Primary Genre Name, Release Date, Track Censored Name, Track Count, Track Explicitness, Track ID, Track Name, Track Number, Track Price, Track Time (Milliseconds)\\
		Open Music&5,558&18&ShelfTitle, AlbumId, AlbumName, AlbumArtwork, AlbumType, AlbumYear, ArtistId, ArtistName, ArtistProfilePhoto, ArtistSubscribers, TrackId, TrackTitle, TrackPlaybackClean, TrackPlaybackExplicit, TrackLength, TrackIndex, TrackViews, TrackFeatures\\
		\hline
		\hline
	\end{tabular}
\end{table}


\subsection{Schema Mapping}
\subsubsection{Design of the Integrated Schema}
The three datasets contain multiple overlapping attributes as seen in Table~\ref{tab2} below. There are 5 attributes within our integrated schema that overlap across at least 2 of 3 input schemata, namely "Artist", "Track Name", "Genre", "Track Duration" and "Release Date".

\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\caption{Table of Integrated Schema Attributes}\label{tab2}
	\centering
	\begin{tabular}{p{4cm}p{3cm}p{6cm}}
		\toprule
		\textbf{Attribute Name} &  \textbf{ Datatype}&\textbf{Datasets in which  attribute found}\\
		\hline
		\hline
		Track&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Artist&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Album&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Youtube Views&decimal&Spotify  Streaming  Statistics, openmusic\\
		YouTube Likes&decimal&Spotify  Streaming  Statistics\\
		Release Date&datetime&Spotify  Streaming  Statistics, openmusic\\
		Danceability&decimal&Spotify  Streaming  Statistics\\
		Energy&decimal&Spotify  Streaming  Statistics\\
		Key&decimal&Spotify  Streaming  Statistics\\
		Loudness&decimal&Spotify  Streaming  Statistics\\
		Speechiness&decimal&Spotify  Streaming  Statistics\\
		Acousticness&decimal&Spotify  Streaming  Statistics\\
		Instrumentalness&decimal&Spotify  Streaming  Statistics\\
		Liveness&decimal&Spotify  Streaming  Statistics\\
		Valence&decimal&Spotify  Streaming  Statistics\\
		Tempo&decimal&Spotify  Streaming  Statistics\\
		TrackPlaybackClean&string&openmusic\\
		TrackPlaybackExplicit&string&openmusic\\
		\hline
		\hline
	\end{tabular}
\end{table}

\subsubsection{Tools and Challenges}
Tools used include Altova MapForce. Challenges encountered during schema mapping included aligning attributes and handling missing data.

\subsubsection{Conversion to Target Schema}
Datasets were converted to the target schema resulting in XML files.

\section{Identity Resolution}
\subsection{Initiate Gold Standard and Challenges}

At the start, we opted for a simple similarity metric---Edit-Distance (\textit{Levenshtein Distance}) using the single key \texttt{Track}---to build the gold standard for the identity resolution phase. Levenshtein Distance is a widely used general-purpose string similarity metric, particularly effective for handling minor typographical errors, such as added, deleted, or substituted characters. For example, it can detect similarities between \textit{"Someone Like You - (Adele Live)"} and \textit{"Someone Like You - Adele"} (similarity 77\%). However, it struggles with reordered components within strings, such as \textit{"Love Song Taylor"} vs. \textit{"Taylor - Love Song"} (similarity 17\%), which can occur in music track titles.

This approach revealed limitations in coverage, as many tracks in the dataset shared the same name but differed in attributes such as \texttt{Album Name}, \texttt{Year}, and \texttt{Artist}, making them distinct. This issue arose from the dataset itself, not from the metrics, as using only the \texttt{Track} key did not capture the necessary information to distinguish between these cases. Additionally, GPT-4o-mini was used to validate 50 randomly selected cases from 1,000, and its results often diverged from manual labeling, further emphasizing the need for more comprehensive matching criteria. Details of this validation process are discussed in the next section.

The gold standard was constructed by extracting and comparing data from \textit{Apple Music}, the \textit{Million Song Dataset}, and \textit{Open Music DB}. Python tools were used to parse XML files and retrieve key attributes such as \texttt{Track} and \texttt{ID}, while random sampling ensured the datasets remained manageable.

While Levenshtein Distance was chosen for its simplicity and typo tolerance, its limited coverage required incorporating additional attributes like \texttt{Artist} for better accuracy. Manual validation helped address mismatches caused by identical track names with different metadata.

Efficiency was improved using \texttt{Dask} for parallel processing and blocking techniques to reduce unnecessary comparisons. However, the resulting gold standard, though sufficient as a baseline, lacked accuracy and required significant refinements to enhance reliability.

\subsection{Gold Standard Improvement}

To enhance the reliability and efficiency of the gold standard, we implemented several improvements that addressed key limitations of the initial approach. These improvements focused on refining similarity matching, reducing computational overhead, and leveraging advanced validation techniques.

\subsubsection*{Similarity Matching Enhancement}
We enhanced the similarity matching process by concatenating \textit{Track Name}, \textit{Artist Name}, and \textit{Year} into a single string. Similarity scores were calculated using normalized Levenshtein metrics, which allowed us to categorize the results into:
\begin{itemize}
    \item \textbf{Matched Cases}: Similarity scores in the range [0.75, 1].
    \item \textbf{Corner Cases}: Similarity scores in the range (0.65, 0.75).
    \item \textbf{Non-Matched Cases}: Similarity scores in the range [0, 0.65].
\end{itemize}

This approach improved differentiation between tracks with similar names but differing metadata, such as different release years or artists.
\subsubsection*{Integration of GPT-4o-mini for Validation}
We incorporated GPT-4o-mini to validate ambiguous and borderline cases. The model processed 1,173 rows from the \textit{OpenDB and Apple Dataset} and 1,510 rows from the \textit{OpenDB and Million Dataset}. By using contextual prompts containing attributes such as \textit{Track Name}, \textit{Artist Name}, and \textit{Year}, GPT-4o-mini resolved complex cases, such as distinguishing live and studio versions of the same track. The example prompt of Apple Dataset and OpenDB Dataset below illustrates the validation process:	
\begin{tcolorbox}[colback=gray!5!white, colframe=black!75!white, title=Prompt]
	You are a music data expert. Compare these music tracks to determine if they represent the same real-world song. \\
	
	\textbf{Data to compare:} \\
	- \texttt{apple\_Artist} vs \texttt{opendb\_Artist} \\
	- \texttt{apple\_Track} vs \texttt{opendb\_Track} \\
	- \texttt{apple\_Album} vs \texttt{opendb\_Album} \\
	- \texttt{apple\_Year} vs \texttt{opendb\_Year} \\
	- \texttt{apple\_Duration} vs \texttt{opendb\_Duration} \\
	
	\textbf{Rules:} \\
	1. Names should be compared case-insensitively, allowing for semantic similarity rather than exact matches. \\
	2. Duration difference should be within ±3000ms. \\
	3. Year difference should be within ±1 year. \\
	4. All attributes must match within thresholds. \\
	
	\textbf{Output:} Return \texttt{1} if the tracks are the same song, or \texttt{0} if they are different.
	\end{tcolorbox}
	
		
\subsubsection*{Stratified Sampling and Manual Review}
We performed stratified sampling using Python's NumPy, ensuring a balanced dataset with a ratio of \textbf{50:30:20} for Non-Matched, Corner, and Matched cases. Additionally, manual review was conducted on similarity computations and GPT evaluations to resolve ambiguities and finalize labeling. This step ensured a robust and high-quality gold standard.

\subsubsection*{Observations and Results}
These refinements resulted in significant improvements:
\begin{itemize}
    \item Reduced computational overhead through blocking and Bloom Filtering.
    \item Improved match accuracy by incorporating additional attributes such as \textit{Artist Name} and \textit{Year}.
    \item Enhanced validation through GPT-4o-mini for resolving corner cases.
\end{itemize}
\textbf{Blocking and Bloom Filtering} were grouped into logical blocks based on the first two characters of the concatenated string to restrict comparisons to records within the same block, significantly reducing the number of pairwise evaluations; additionally, a Bloom filter was applied within each block to efficiently pre-filter records, ensuring that only likely matches were compared, which optimized the process by reducing pairwise comparisons by approximately 70% while maintaining a low false positive rate of 0.1%.

The refined gold standard is robust and ready for the identity resolution phase, with further confirmation expected after data fusion.

\subsection{Matching Strategies}
\subsubsection{Blocking Methods}

To reduce unnecessary comparisons during identity resolution (IR), we generated blocking keys based on track names. The blocking key was derived from the bigrams of the first three tokens of each track name. We then experimented with two blocking methods: \textbf{Standard Blocking} and the \textbf{Sorted Neighborhood Method}, both using these track name-based blocking keys.

In the case of running IR for the apple + opendb datasets, the maximum number of entities sharing the same hashed blocking key was 9.696 (for the blocking key "CH"), while the minimum was 1 (for the blocking key "LAINTH"). As for the million + opendb datasets, the maximum number of entities sharing the same hashed blocking key was 67.404 (for the blocking key "RE"), while the minimum was 1.560 (for the blocking key "BL"). When using the Sorted Neighborhood Method, the window size would need to be greater than 9.696 and 67.404 respectively to ensure that no matches were missed. However, such a large window size would significantly increase resource consumption by comparing many irrelevant records.

Given this inefficiency, for both of our entity matching comparisons, we ultimately decided to adopt \textbf{Standard Blocking}, which efficiently grouped entities based on their blocking keys without requiring extensive resource expenditure or risking lost matches.
\subsubsection{Similarity Metrics}
As mentioned above, in the entity matching for apple + opendb, we use \textbf{track name, artist name, album name, }and \textbf{album year} as the basis for determining entity matching. As for entity matching for million + opendb, since there is no album name attribute in million dataset, we only use \textbf{track name, artist name}, and \textbf{album year} as the basis. For each attribute with a data type of string, we tested various metrics, including edit-based (Levenshtein, Jaro, Jaro-Winkler), token-based (Jaccard), and phonetic (Soundex). For numeric attribute, we use absolute difference of 2 years.

\subsection{Evaluation}
\subsubsection{Metrics and Analysis}

After testing over 50+ combinations of comparators for entity matching between apple + opendb datasets and million + opendb datasets, the metrics shown in Table~\ref{tab:comparators} were found to be the most suitable for their respective attributes.

% Adjust padding
\setlength{\tabcolsep}{4pt} % Horizontal padding
\renewcommand{\arraystretch}{1.5} % Vertical padding
\begin{table}[ht]
	\centering
	\caption{Comparators Used for Entity Matching Across Datasets}
	\label{tab:comparators}
	\begin{tabular}{p{2.5cm}p{2.5cm}p{6cm}}
	\hline
	\textbf{Dataset}      & \textbf{Attribute (Comparator)}            & \textbf{Reason for Effectiveness}                                                                                      \\ \hline
	apple+opendb; million+opendb & \raggedright Track Name (Jaccard)              & Track names often include variations like additional descriptors, e.g., single, feat.                                  \\ \hline
	apple+opendb                  & \raggedright Artist Name (Jaro-Winkler)                  & Artist names are short and structured. Most typos occur in the last name rather than the first name.                    \\ \hline
	million+opendb                & \raggedright Artist Name (Equal)                          & Many entities in million + opendb have the same track name but different artist names. Within those entities, the artist names are quite similar in some cases; therefore, the equal comparator is needed to clearly distinguish ambiguous entities. \\ \hline
	apple+opendb                  & \raggedright Album Name (Jaccard)              & Most of the singles use the track name as their album name, so the same pattern applies here.                           \\ \hline
	apple+opendb; million+opendb & \raggedright Album Year (Absolute Difference ±2)       & Accounts for real-world scenarios like re-releases and recording/release year discrepancies.                            \\ \hline
	\end{tabular}
	\end{table}

	We organized some of our IR tests in Table~\ref{tab:benchmark}. Those combination we finally chose are highlighted in yellow. In the results for apple + opendb, we intuitively selected the one that demonstrated the best performance across Precision, Recall, and F1-score. Upon closer inspection of the correspondences, the matches were indeed highly accurate, confirming our choice.

	However, in the results for million + opendb, after reviewing the correspondences, we decided to select the option that did not achieve the best performance metrics. This decision was based on the presence of more ambiguous data in this comparison, which, combined with an insufficient golden standard to accurately use the album year to distinguish different entities, resulted in false positives within the correspondences. Under these circumstances, the selected option demonstrated the most balanced precision and recall, minimizing false positives while still capturing the majority of true matches. This balance made it the optimal choice for this dataset.

\begin{table}[]
	\small
	\caption{Entity Matching Benchmark Table}
	\label{tab:benchmark}
	\begin{threeparttable}
	\begin{tabular}{lp{4.5cm}p{2cm}ccccc}
		\hline
		\textbf{Datasets}      & \textbf{Matching Rule}                                          & \textbf{B}                          & \textbf{P} & \textbf{R} & \textbf{F1}  & \textbf{\# Corr}  \\\hline
		apple+opendb         & \raggedright Track (J*), Artist (JW*), \\ Album (J), Album Year (2Y*) & \raggedright SNB Track (20)              & 0,99              & 0,66          & 0,79       & 85               \\
		apple+opendb         & \raggedright Track (J), Artist (JW), \\ Album (J), Album Year (2Y) & \raggedright SNB Track (60)              & 0,99             & 0,82          & 0,89       & 105              \\
		\rowcolor[HTML]{FFFFCC} 
		apple+opendb         & \raggedright Track (J), Artist (JW), \\ Album (J), Album Year (2Y) & \raggedright Standard Track                            & 0,99             & 0,93          & 0,96       & 120              \\
		apple+opendb         & \raggedright Track (L), Artist (JW), \\ Album (L*), Album Year (2Y) & \raggedright Standard Track                            & 0,99             & 0,92           & 0,9544       & 118              \\
		apple+opendb         & \raggedright Track (S*), Artist (JW),\\  Album (S), Album Year (2Y) & \raggedright Standard Track                            & 0,97             & 0,95          & 0,96       & 170              \\
		apple+opendb         & \raggedright Track (J), Artist (J),\\  Album (J)             & \raggedright Standard Track                            & 0,87             & 0,94          & 0,91       & 291              \\ \hline\hline
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright SNB Track (20)              & 0,94             & 0,72         & 0,82       & 684              \\
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright SNB Track (60)              & 0,92             & 0,85         & 0,88       & 835              \\
		\rowcolor[HTML]{FFFFCC} 
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright Standard Track                            & 0,92             & 0,94         & 0,93      & 942              \\
		million+opendb       & \raggedright Track (J), Artist (JW), \\ Album Year (2Y)     & \raggedright Standard Track                            & 0,91             & 0,99         & 0,95       & 2.078            \\ 
		million+opendb       & \raggedright Track (S), Artist (JW), \\ Album Year (2Y)     & \raggedright Standard Track                            & 0,88             & 0,94         & 0,91       & 1.326            \\
		million+opendb       & \raggedright Track (J), Artist (Equal)                                & \raggedright Standard Track                            & 0,88             & 0,94         & 0,91      & 1.082            \\ \hline\hline
	\end{tabular}
	\begin{tablenotes}
		\footnotesize
		\item \textbf{*Note:} J: Jaccard; JW: Jaro-Winkler; 2Y: Absolute Difference ±2; S: Soundex; L: Levenshtein
		\end{tablenotes}
	\end{threeparttable}
\end{table}

Although the data in the table suggests that the results of Entity Matching are reasonably satisfactory, a closer examination of the million + opendb correspondences reveals that the differentiation of entities based on the album year is insufficient. This limitation is also reflected in the results of our Group Size analysis.
\setlength{\tabcolsep}{4pt} % Horizontal padding
\begin{table}[ht]
	\centering
	\caption{Group Size Analysis}
	\label{tab:group_size}
	\begin{tabular}{lccccc}
	\hline
	\rowcolor[HTML]{D9D9D9} 
	\textbf{Group Size} & \textbf{2} & \textbf{3} & \textbf{4-7} & \textbf{8-13} & \textbf{14} \\ \hline
	\textbf{Frequency}  & 650        & 104        & 45           & 4            & 0           \\ \hline
	\textbf{Distribution} & 81\%       & 13\%       & 6\%          & 0\%           & 0\%         \\ \hline
	\end{tabular}
	\end{table}
	

\subsubsection{Error That Remain}

Our IR results for album years continue to show inconsistencies, with mismatched or slightly inaccurate years persisting even when using a tolerance of ±2 years. These discrepancies are particularly evident when dealing with re-releases, remasters, or cases where album metadata varies significantly across datasets.

We think that the root cause of this issue lies in the golden standard we employed, which was derived by concatenating track name, artist name, and album year into a single string and performing an initial comparison using the edit-based Levenshtein metric. Since album years were not accurately distinguished in this process, it led to the observed inconsistencies. This misalignment in album years will negatively impact attribute consistency during the Data Fusion phase. If this issue remains unresolved, the resulting fused data will likely inherit these inconsistencies, compromising its overall quality and trustworthiness.
\section{Data Fusion}
\subsection{Fusion Rules}
\subsubsection{Conflict Resolution Strategies}
Conflict resolution strategies included prioritizing reliable datasets, averaging numeric attributes, and using union for lists.

\subsubsection{Specific Fusion Rules}
Specific fusion rules for key attributes included taking the shortest string for "Track Name" and averaging for "Streams".

\subsection{Fused Data Output}
\subsubsection{Post-Fusion Dataset}
Post-fusion dataset size and density improvements were noted. Examples of fused records were provided.

\subsection{Quality Evaluation}
\subsubsection{Metrics for Evaluation}
Metrics for evaluating the quality of the integrated dataset included accuracy, consistency, and density. Summary of improvements achieved through data integration was provided.

\subsection{Challenges and Lessons Learned}
Addressed issues like conflicting values, missing data, or incorrect matches from identity resolution.

\section{Conclusion and Future Work}
\subsection{Limitations of the Project}
Discussed limitations such as incomplete data and computational constraints.

\subsection{Recommendations for Future Improvements}
Recommendations included using advanced ML models for identity resolution.

\bibliographystyle{plainurl}
\bibliography{document.bib} 
\end{document}
