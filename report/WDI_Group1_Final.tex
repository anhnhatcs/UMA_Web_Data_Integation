% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
\usepackage{cite}
\usepackage{booktabs}
\usepackage[colorlinks, linkcolor=blue]{hyperref}
\usepackage[table]{xcolor}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{threeparttable}
\usepackage{array} % For table enhancements
\setcounter{secnumdepth}{3}
\makeatletter
\def\UrlAlphabet{%
	\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j%
	\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t%
	\do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D%
	\do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N%
	\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X%
	\do\Y\do\Z}
\def\UrlDigits{\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\0}
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\g@addto@macro{\UrlBreaks}{\UrlAlphabet}
\g@addto@macro{\UrlBreaks}{\UrlDigits}
\makeatother
%
\begin{document}
	%
	\pagestyle{plain} % or \pagestyle{headings} for headers
	
	\title{Web Data Integration\\Open Music Data Integration}
	%
	%\titlerunning{Abbreviated paper title}
	% If the paper title is too long for the running head, you can set
	% an abbreviated paper title here
	%
	\author{Anh-Nhat Nguyen\orcidID{2034311} \and
		Ching-Yun Cheng\orcidID{2112322}\and
		Shamalan Rajesvaran\orcidID{2115475} \and 
		Yen-An Chen\orcidID{2113612} \and 
		Phelan Lee Yeuk Bun\orcidID{2053019} }
	%
	\authorrunning{Anh-Nhat Nguyen et al.}
	% First names are abbreviated in the running head.
	% If there are more than two authors, 'et al.' is used.
	%
	\institute{Team 1}
	%\and
	%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
	%\email{lncs@springer.com}\\
	%\url{http://www.springer.com/gp/computer-science/lncs} \and
	%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
	%\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\section{Introduction}
In this Web Data Integration project, we aim to consolidate and analyze standardization from multiple sources to gain comprehensive insights into music streaming trends, track performance, and audience preferences. The project leverages three key datasets:

1. Million Song Dataset with Spotify and Last.fm Features Dataset (csv) \cite{Million_Spot_LastFM}: This dataset is an enriched version of the Million Song Dataset, a large-scale music database containing detailed metadata and audio features for over 50,000 tracks with 21 attributes. It integrates additional attributes from Spotify and Last.fm, including audio features like danceability, energy, loudness, and popularity metrics such as tags (list attribute), preview URLs, and genre classifications. The merging of these three data sources provides a comprehensive view of each song, making it suitable for analyzing music trends, listener behaviors, and track popularity across platforms.

2. Apple Music Tracks (csv) \cite{Apple_Music}: This dataset contains detailed information on 10,000 tracks with 24 attributes sourced from Apple Music. It includes attributes such as artist names, album titles, track features (e.g., tempo, key, mode), and genre classifications. In addition to audio metadata, the dataset provides insights into song popularity metrics and trends across the platform. It is ideal for exploring the characteristics of songs on Apple Music, understanding artist performance, and analyzing trends in genres and musical features.

3. Openmusic API Dataset (json) \cite{open_music}: This dataset offers details about over 5,500 tracks via web APIs (retrieved by /explore \& /album?id=<AlbumID>), including track and album metadata, artist information, and playback types (clean/explicit). The use case for this dataset revolves around leveraging real-time API data for in-depth analysis of track consumption patterns, artist popularity, and changes in audience preferences over time.

Together, these datasets will help provide a holistic view of how various musical, commercial, and audience factors contribute to the success of music tracks on different platforms based on various algorithm approach \cite{doan_principles_2012}.

\section{Data Collection and Data Translation}
\subsection{Data Collection and Dataset}
\subsubsection{Overview of the Datasets}
The dataset was obtained from Kaggle in the form of $csv$ and OpenMusic API in the form of $csv$ and $json$. An overview of dataset attributes is presented in Table~\ref{tab1} below.

\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\caption{Dataset structure}\label{tab1}
	\centering
	\begin{tabular}{p{3cm} p{3cm}p{3cm}p{5cm}}
		\toprule
		\textbf{Dataset} &  \textbf{ No Entities}&\textbf{No. Attributes}& \textbf{Attributes}\\
		\hline
		\hline
		Million Song Dataset with Spotify and Last.fm Features&50,683&21&Track ID, Name, Artist, Spotify Preview URL, Spotify ID, Tags, Genre (\emph{MV 56\%}), Year, Duration MS, Danceability, Energy, Key, Loudness, Mode, Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo, Time Signature\\
		Apple Music Tracks&10,000&24
		&Artist ID, Artist Name, Collection Censored Name, Collection ID, Collection Name, Collection Price, Content Advisory Rating (\emph{MV 85\%}), Country, Currency, Disc Count, Disc Number, Is Streamable, Kind, Preview URL, Primary Genre Name, Release Date, Track Censored Name, Track Count, Track Explicitness, Track ID, Track Name, Track Number, Track Price, Track Time (Milliseconds)\\
		Open Music&5,558&18&ShelfTitle, AlbumId, AlbumName, AlbumArtwork, AlbumType, AlbumYear, ArtistId, ArtistName, ArtistProfilePhoto, ArtistSubscribers, TrackId, TrackTitle, TrackPlaybackClean, TrackPlaybackExplicit, TrackLength, TrackIndex, TrackViews, TrackFeatures\\
		\hline
		\hline
	\end{tabular}
\end{table}

\subsubsection{Data Preprocessing}
Data preprocessing steps included filtering irrelevant attributes and normalizing data types.

\subsection{Schema Mapping}
\subsubsection{Design of the Integrated Schema}
The three datasets contain multiple overlapping attributes as seen in Table~\ref{tab2} below. There are 5 attributes within our integrated schema that overlap across at least 2 of 3 input schemata, namely "Artist", "Track Name", "Genre", "Track Duration" and "Release Date".

\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\caption{Table of Integrated Schema Attributes}\label{tab2}
	\centering
	\begin{tabular}{p{4cm}p{3cm}p{6cm}}
		\toprule
		\textbf{Attribute Name} &  \textbf{ Datatype}&\textbf{Datasets in which  attribute found}\\
		\hline
		\hline
		Track&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Artist&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Album&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Youtube Views&decimal&Spotify  Streaming  Statistics, openmusic\\
		YouTube Likes&decimal&Spotify  Streaming  Statistics\\
		Release Date&datetime&Spotify  Streaming  Statistics, openmusic\\
		Danceability&decimal&Spotify  Streaming  Statistics\\
		Energy&decimal&Spotify  Streaming  Statistics\\
		Key&decimal&Spotify  Streaming  Statistics\\
		Loudness&decimal&Spotify  Streaming  Statistics\\
		Speechiness&decimal&Spotify  Streaming  Statistics\\
		Acousticness&decimal&Spotify  Streaming  Statistics\\
		Instrumentalness&decimal&Spotify  Streaming  Statistics\\
		Liveness&decimal&Spotify  Streaming  Statistics\\
		Valence&decimal&Spotify  Streaming  Statistics\\
		Tempo&decimal&Spotify  Streaming  Statistics\\
		TrackPlaybackClean&string&openmusic\\
		TrackPlaybackExplicit&string&openmusic\\
		\hline
		\hline
	\end{tabular}
\end{table}

\subsubsection{Tools and Challenges}
Tools used include Altova MapForce. Challenges encountered during schema mapping included aligning attributes and handling missing data.

\subsubsection{Conversion to Target Schema}
Datasets were converted to the target schema resulting in XML files.

\section{Phase II: Identity Resolution}
\subsection{Initiate Gold Standard}
\subsubsection{Method for Building the Gold Standard}
The gold standard was built using a sampling method ensuring matches, non-matches, and corner cases.

\subsection{Challenges with Gold Standard and Improvement}
\subsubsection{Edit-Distance with Single Key}
Edit-distance with a single key (Track) did not have high coverage. Solutions included using multiple keys and advanced matching techniques.

\subsubsection{Selection Bias}
Addressed selection bias by ensuring diverse and representative samples.

\subsubsection{Performance Improvement}
Implemented blocking, Bloom Filtering, and used LLM for performance improvement.

\subsection{Matching Strategies}
\subsubsection{Blocking Methods}

To reduce unnecessary comparisons during identity resolution (IR), we generated blocking keys based on track names. The blocking key was derived from the bigrams of the first three tokens of each track name. We then experimented with two blocking methods: \textbf{Standard Blocking} and the \textbf{Sorted Neighborhood Method}, both using these track name-based blocking keys.

In the case of running IR for the apple + opendb datasets, the maximum number of entities sharing the same hashed blocking key was 9.696 (for the blocking key "CH"), while the minimum was 1 (for the blocking key "LAINTH"). As for the million + opendb datasets, the maximum number of entities sharing the same hashed blocking key was 67.404 (for the blocking key "RE"), while the minimum was 1.560 (for the blocking key "BL"). When using the Sorted Neighborhood Method, the window size would need to be greater than 9.696 and 67.404 respectively to ensure that no matches were missed. However, such a large window size would significantly increase resource consumption by comparing many irrelevant records.

Given this inefficiency, for both of our entity matching comparisons, we ultimately decided to adopt \textbf{Standard Blocking}, which efficiently grouped entities based on their blocking keys without requiring extensive resource expenditure or risking lost matches.
\subsubsection{Similarity Metrics}
As mentioned above, in the entity matching for apple + opendb, we use \textbf{track name, artist name, album name, }and \textbf{album year} as the basis for determining entity matching. As for entity matching for million + opendb, since there is no album name attribute in million dataset, we only use \textbf{track name, artist name}, and \textbf{album year} as the basis. For each attribute with a data type of string, we tested various metrics, including edit-based (Levenshtein, Jaro, Jaro-Winkler), token-based (Jaccard), and phonetic (Soundex). For numeric attribute, we use absolute difference of 2 years.

\subsection{Evaluation}
\subsubsection{Metrics and Analysis}

After testing over 50+ combinations of comparators for entity matching between apple + opendb datasets and million + opendb datasets, the metrics shown in Table~\ref{tab:comparators} were found to be the most suitable for their respective attributes.

% Adjust padding
\setlength{\tabcolsep}{4pt} % Horizontal padding
\renewcommand{\arraystretch}{1.5} % Vertical padding
\begin{table}[ht]
	\centering
	\caption{Comparators Used for Entity Matching Across Datasets}
	\label{tab:comparators}
	\begin{tabular}{p{2.5cm}p{2.5cm}p{6cm}}
	\hline
	\textbf{Dataset}      & \textbf{Attribute (Comparator)}            & \textbf{Reason for Effectiveness}                                                                                      \\ \hline
	apple+opendb; million+opendb & \raggedright Track Name (Jaccard)              & Track names often include variations like additional descriptors, e.g., single, feat.                                  \\ \hline
	apple+opendb                  & \raggedright Artist Name (Jaro-Winkler)                  & Artist names are short and structured. Most typos occur in the last name rather than the first name.                    \\ \hline
	million+opendb                & \raggedright Artist Name (Equal)                          & Many entities in million + opendb have the same track name but different artist names. Within those entities, the artist names are quite similar in some cases; therefore, the equal comparator is needed to clearly distinguish ambiguous entities. \\ \hline
	apple+opendb                  & \raggedright Album Name (Jaccard)              & Most of the singles use the track name as their album name, so the same pattern applies here.                           \\ \hline
	apple+opendb; million+opendb & \raggedright Album Year (Absolute Difference ±2)       & Accounts for real-world scenarios like re-releases and recording/release year discrepancies.                            \\ \hline
	\end{tabular}
	\end{table}

	We organized some of our IR tests in Table~\ref{tab:benchmark}. Those combination we finally chose are highlighted in yellow. In the results for apple + opendb, we intuitively selected the one that demonstrated the best performance across Precision, Recall, and F1-score. Upon closer inspection of the correspondences, the matches were indeed highly accurate, confirming our choice.

	However, in the results for million + opendb, after reviewing the correspondences, we decided to select the option that did not achieve the best performance metrics. This decision was based on the presence of more ambiguous data in this comparison, which, combined with an insufficient golden standard to accurately use the album year to distinguish different entities, resulted in false positives within the correspondences. Under these circumstances, the selected option demonstrated the most balanced precision and recall, minimizing false positives while still capturing the majority of true matches. This balance made it the optimal choice for this dataset.

\begin{table}[]
	\small
	\caption{Entity Matching Benchmark Table}
	\label{tab:benchmark}
	\begin{threeparttable}
	\begin{tabular}{lp{4.5cm}p{2cm}ccccc}
		\hline
		\textbf{Datasets}      & \textbf{Matching Rule}                                          & \textbf{B}                          & \textbf{P} & \textbf{R} & \textbf{F1}  & \textbf{\# Corr}  \\\hline
		apple+opendb         & \raggedright Track (J*), Artist (JW*), \\ Album (J), Album Year (2Y*) & \raggedright SNB Track (20)              & 0,99              & 0,66          & 0,79       & 85               \\
		apple+opendb         & \raggedright Track (J), Artist (JW), \\ Album (J), Album Year (2Y) & \raggedright SNB Track (60)              & 0,99             & 0,82          & 0,89       & 105              \\
		\rowcolor[HTML]{FFFFCC} 
		apple+opendb         & \raggedright Track (J), Artist (JW), \\ Album (J), Album Year (2Y) & \raggedright Standard Track                            & 0,99             & 0,93          & 0,96       & 120              \\
		apple+opendb         & \raggedright Track (L), Artist (JW), \\ Album (L*), Album Year (2Y) & \raggedright Standard Track                            & 0,99             & 0,92           & 0,9544       & 118              \\
		apple+opendb         & \raggedright Track (S*), Artist (JW),\\  Album (S), Album Year (2Y) & \raggedright Standard Track                            & 0,97             & 0,95          & 0,96       & 170              \\
		apple+opendb         & \raggedright Track (J), Artist (J),\\  Album (J)             & \raggedright Standard Track                            & 0,87             & 0,94          & 0,91       & 291              \\ \hline\hline
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright SNB Track (20)              & 0,94             & 0,72         & 0,82       & 684              \\
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright SNB Track (60)              & 0,92             & 0,85         & 0,88       & 835              \\
		\rowcolor[HTML]{FFFFCC} 
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright Standard Track                            & 0,92             & 0,94         & 0,93      & 942              \\
		million+opendb       & \raggedright Track (J), Artist (JW), \\ Album Year (2Y)     & \raggedright Standard Track                            & 0,91             & 0,99         & 0,95       & 2.078            \\ 
		million+opendb       & \raggedright Track (S), Artist (JW), \\ Album Year (2Y)     & \raggedright Standard Track                            & 0,88             & 0,94         & 0,91       & 1.326            \\
		million+opendb       & \raggedright Track (J), Artist (Equal)                                & \raggedright Standard Track                            & 0,88             & 0,94         & 0,91      & 1.082            \\ \hline\hline
	\end{tabular}
	\begin{tablenotes}
		\footnotesize
		\item \textbf{*Note:} J: Jaccard; JW: Jaro-Winkler; 2Y: Absolute Difference ±2; S: Soundex; L: Levenshtein
		\end{tablenotes}
	\end{threeparttable}
\end{table}

Although the data in the table suggests that the results of Entity Matching are reasonably satisfactory, a closer examination of the million + opendb correspondences reveals that the differentiation of entities based on the album year is insufficient. This limitation is also reflected in the results of our Group Size analysis.
\setlength{\tabcolsep}{4pt} % Horizontal padding
\begin{table}[ht]
	\centering
	\caption{Group Size Analysis}
	\label{tab:group_size}
	\begin{tabular}{lccccc}
	\hline
	\rowcolor[HTML]{D9D9D9} 
	\textbf{Group Size} & \textbf{2} & \textbf{3} & \textbf{4-7} & \textbf{8-13} & \textbf{14} \\ \hline
	\textbf{Frequency}  & 650        & 104        & 45           & 4            & 0           \\ \hline
	\textbf{Distribution} & 81\%       & 13\%       & 6\%          & 0\%           & 0\%         \\ \hline
	\end{tabular}
	\end{table}
	

\subsubsection{Error That Remain}

Our IR results for album years continue to show inconsistencies, with mismatched or slightly inaccurate years persisting even when using a tolerance of ±2 years. These discrepancies are particularly evident when dealing with re-releases, remasters, or cases where album metadata varies significantly across datasets.

We think that the root cause of this issue lies in the golden standard we employed, which was derived by concatenating track name, artist name, and album year into a single string and performing an initial comparison using the edit-based Levenshtein metric. Since album years were not accurately distinguished in this process, it led to the observed inconsistencies. This misalignment in album years will negatively impact attribute consistency during the Data Fusion phase. If this issue remains unresolved, the resulting fused data will likely inherit these inconsistencies, compromising its overall quality and trustworthiness.
\section{Data Fusion}
\subsection{Fusion Rules}
\subsubsection{Conflict Resolution Strategies}
In the data fusion process, conflict resolution strategies are essential to derive the most accurate and reliable representation of records when combining multiple datasets. Our conflict resolution strategies include
using reliable datasets with sufficient attribute intersection for fusion, as well as strong identity resolution results that maintain quality of data. We decided to use all of our datasets (apple, million, opendb) as
they fulfil our prerequisites with IR results as shown above. We had assigned a provenance score to each dataset in order to reflect its reliability and accuracy. 'apple' dataset was given the highest score (3.0), followed by 'million' (2.0), and then 'opendb' (1.0).
This scoring system allowed the fusion process to favor attributes from the more reliable datasets when resolving conflicts. 

\subsubsection{Specific Fusion Rules}
Our team have included a specific fusion rules for resolving conflicts in key attributes to ensure that the fused dataset accurately reflects the most reliable, comprehensive, and meaningful information. These rules are tailored to the nature of each attribute and the type of conflicts typically encountered.
For the track names, album names and artist names, the longest string strategy is used to resolve conflicts. This approach ensures that the most descriptive and complete track title is chosen, as longer names often include additional context, such as versions or remixes. Likewise, this fusion rule will
retain information such as featuring artists or collaborations for the artist names.

For the album year, a voting strategy is employed for resolving conflicts. This strategy aggregates values across the 3 datasets and selects the most commonly occurring year, reflecting a consensus among the sources. This minimizes the impact of outliers or errors in individual datasets and ensures consistency.

The favor source with the highest provenance score strategy is used for the track duration, avoiding discrepancies caused by slight rounding differences or errors in less reliable sources.

\subsection{Fused Data Output}
\subsubsection{Post-Fusion Dataset}
Post-fusion dataset size and density improvements were noted. Examples of fused records were provided.

\subsection{Quality Evaluation}
We conducted extensive testing with various combinations of fusion methods. Table \ref{tab:fusion-strategies} shows four representative strategies that combine different fusion methods, including LongestString(), FavourSource(), ShortestString(), and MostRecent(). Strategy 4 demonstrated the highest accuracy at 84\% and was therefore selected as our optimized fusion strategy.

\begin{table}[h!]
	\setlength{\tabcolsep}{4pt}
	\small
	\caption{Data Fusion Strategy Comparison}
	\label{tab:fusion-strategies}
	\begin{threeparttable}
	\begin{tabular}{p{2.5cm}p{7.5cm}c}
	\hline
	\textbf{Fusion Strategy} & \textbf{Method} & \textbf{Accuracy} \\ \hline
	Strategy 1 & \raggedright ArtistFuserLongestString(), \newline
	AlbumYearFuserVoting(), \newline
	AlbumFuserLongestString(), \newline
	DurationFuserFavourSource(), \newline
	TitleFuserLongestString(), \newline
	ExplicitnessFuserFavourSource() & 80\% \\
	\hline
	Strategy 2 & \raggedright ArtistFuserFavourSource(), \newline
	AlbumYearFuserFavourSource(), \newline
	AlbumFuserFavourSource(), \newline
	DurationFuserFavourSource(), \newline
	TitleFuserFavourSource(), \newline
	ExplicitnessFuserFavourSource() & 80\% \\
	\hline
	Strategy 3 & \raggedright ArtistFuserShortestString(), \newline
	AlbumYearFuserVoting(), \newline
	AlbumFuserShortestString(), \newline
	DurationFuserFavourSource(), \newline
	TitleFuserShortestString(), \newline
	ExplicitnessFuserFavourSource() & 82\% \\
	\hline
	\rowcolor[HTML]{FFFFCC}
	Strategy 4 \newline (Optimized) & \raggedright ArtistFuserShortestString(), \newline
	AlbumYearFuserMostRecent(), \newline
	AlbumFuserShortestString(), \newline
	DurationFuserMostRecent(), \newline
	TitleFuserShortestString(), \newline
	ExplicitnessFuserMostRecent() & 84\% \\ \hline
	\end{tabular}
	\end{threeparttable}
	\end{table}

	To evaluate our optimized fusion strategy's effectiveness, we analyzed its performance across different attributes (Table \ref{table:optimized_fusion_rules}). While most attributes achieved high accuracy, Album Name information showed notably low accuracy (50\%) due to the frequent occurrence of the same track appearing in different album releases (e.g., singles, original albums, compilation albums). This common practice in the music industry creates challenges in determining the canonical album name for a track, leading lower accuracy and consistency scores for Album Name (50\%, 81\%) and AlbumYear (75\%, 56\%) attributes.

	\begin{table}[h!]
		\centering
		\caption{Optimized Fusion Strategy Performance}
		\label{table:optimized_fusion_rules}
		\begin{tabular}{llccc}
		\toprule
		\textbf{Attribute} & \textbf{Method} & \textbf{Accuracy} & \textbf{Consistency} & \textbf{Density} \\ 
		\midrule
		Artist             & ArtistFuserShortestString()           & 95\%              & 100\%                & 100\%            \\ 
		AlbumYear        & AlbumYearFuserMostRecent()           & 75\%              & 56\%                 & 100\%            \\ 
		Album              & AlbumFuserShortestString()           & 50\%              & 81\%                 & 100\%            \\ 
		Duration           & DurationFuserMostRecent()            & 85\%              & 88\%                 & 100\%            \\ 
		Track              & TitleFuserShortestString()           & 85\%              & 98\%                 & 100\%            \\ 
		TrackExplicitness & ExplicitnessFuserMostRecent()        & 90\%              & 99\%                 & 100\%            \\ 
		\bottomrule
		\end{tabular}
		\end{table}
		

\section{Conclusion and Future Work}
\subsection{Limitations of the Project}
Discussed limitations such as incomplete data and computational constraints.

\subsection{Recommendations for Future Improvements}
Recommendations included using advanced ML models for identity resolution.

\bibliographystyle{plainurl}
\bibliography{document.bib} 
\end{document}
