% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
\usepackage{cite}
\usepackage{booktabs}
\usepackage[colorlinks, linkcolor=blue]{hyperref}
\usepackage[table]{xcolor}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{threeparttable}
\usepackage{array} % For table enhancements
\usepackage[most]{tcolorbox}

\setcounter{secnumdepth}{3}
\makeatletter
\def\UrlAlphabet{%
	\do\a\do\b\do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j%
	\do\k\do\l\do\m\do\n\do\o\do\p\do\q\do\r\do\s\do\t%
	\do\u\do\v\do\w\do\x\do\y\do\z\do\A\do\B\do\C\do\D%
	\do\E\do\F\do\G\do\H\do\I\do\J\do\K\do\L\do\M\do\N%
	\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V\do\W\do\X%
	\do\Y\do\Z}
\def\UrlDigits{\do\1\do\2\do\3\do\4\do\5\do\6\do\7\do\8\do\9\do\0}
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\g@addto@macro{\UrlBreaks}{\UrlAlphabet}
\g@addto@macro{\UrlBreaks}{\UrlDigits}
\makeatother
%
\begin{document}
	%
	\pagestyle{plain} % or \pagestyle{headings} for headers
	
	\title{Web Data Integration\\Open Music Data Integration}
	%
	%\titlerunning{Abbreviated paper title}
	% If the paper title is too long for the running head, you can set
	% an abbreviated paper title here
	%
	\author{Anh-Nhat Nguyen\orcidID{2034311} \and
		Ching-Yun Cheng\orcidID{2112322}\and
		Shamalan Rajesvaran\orcidID{2115475} \and 
		Yen-An Chen\orcidID{2113612} \and 
		Phelan Lee Yeuk Bun\orcidID{2053019} }
	%
	\authorrunning{Anh-Nhat Nguyen et al.}
	% First names are abbreviated in the running head.
	% If there are more than two authors, 'et al.' is used.
	%
	\institute{University of Mannheim, Germany - Team 1} 
	%\and
	%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
	%\email{lncs@springer.com}\\
	%\url{http://www.springer.com/gp/computer-science/lncs} \and
	%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
	%\email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\section{Introduction}
In this Web Data Integration project, we aim to consolidate and analyze standardization from multiple sources to gain comprehensive insights into music streaming trends, track performance, and audience preferences. The project leverages three key datasets:

1. Million Song Dataset with Spotify and Last.fm Features Dataset (csv) \cite{Million_Spot_LastFM}: This dataset is an enriched version of the Million Song Dataset, a large-scale music database containing detailed metadata and audio features for over 50,000 tracks with 21 attributes. It integrates additional attributes from Spotify and Last.fm, including audio features like danceability, energy, loudness, and popularity metrics such as tags (list attribute), preview URLs, and genre classifications. The merging of these three data sources provides a comprehensive view of each song, making it suitable for analyzing music trends, listener behaviors, and track popularity across platforms.

2. Apple Music Tracks (csv) \cite{Apple_Music}: This dataset contains detailed information on 10,000 tracks with 24 attributes sourced from Apple Music. It includes attributes such as artist names, album titles, track features (e.g., tempo, key, mode), and genre classifications. In addition to audio metadata, the dataset provides insights into song popularity metrics and trends across the platform. It is ideal for exploring the characteristics of songs on Apple Music, understanding artist performance, and analyzing trends in genres and musical features.

3. Openmusic API Dataset (json) \cite{open_music}: This dataset offers details about over 5,500 tracks via web APIs (retrieved by /explore \& /album?id=<AlbumID>), including track and album metadata, artist information, and playback types (clean/explicit). The use case for this dataset revolves around leveraging real-time API data for in-depth analysis of track consumption patterns, artist popularity, and changes in audience preferences over time.

Together, these datasets will help provide a holistic view of how various musical, commercial, and audience factors contribute to the success of music tracks on different platforms based on various algorithm approach \cite{doan_principles_2012}.

\section{Data Collection and Data Translation}
\subsection{Data Collection and Dataset}
The dataset was obtained from Kaggle in the form of $csv$ and OpenMusic API in the form of $csv$ and $json$. An overview of dataset attributes is presented in Table~\ref{tab1} below.

\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\caption{Dataset structure}\label{tab1}
	\centering
	\begin{tabular}{p{3cm} p{3cm}p{3cm}p{5cm}}
		\toprule
		\textbf{Dataset} &  \textbf{ No Entities}&\textbf{No. Attributes}& \textbf{Attributes}\\
		\hline
		\hline
		Million Song Dataset with Spotify and Last.fm Features&50,683&21&Track ID, Name, Artist, Spotify Preview URL, Spotify ID, Tags, Genre (\emph{MV 56\%}), Year, Duration MS, Danceability, Energy, Key, Loudness, Mode, Speechiness, Acousticness, Instrumentalness, Liveness, Valence, Tempo, Time Signature\\
		Apple Music Tracks&10,000&24
		&Artist ID, Artist Name, Collection Censored Name, Collection ID, Collection Name, Collection Price, Content Advisory Rating (\emph{MV 85\%}), Country, Currency, Disc Count, Disc Number, Is Streamable, Kind, Preview URL, Primary Genre Name, Release Date, Track Censored Name, Track Count, Track Explicitness, Track ID, Track Name, Track Number, Track Price, Track Time (Milliseconds)\\
		Open Music&5,558&18&ShelfTitle, AlbumId, AlbumName, AlbumArtwork, AlbumType, AlbumYear, ArtistId, ArtistName, ArtistProfilePhoto, ArtistSubscribers, TrackId, TrackTitle, TrackPlaybackClean, TrackPlaybackExplicit, TrackLength, TrackIndex, TrackViews, TrackFeatures\\
		\hline
		\hline
	\end{tabular}
\end{table}


\subsection{Schema Mapping}
\subsubsection{Design of the Integrated Schema}
The three datasets contain multiple overlapping attributes as seen in Table~\ref{tab2} below. There are 5 attributes within our integrated schema that overlap across at least 2 of 3 input schemata, namely "Artist", "Track Name", "Genre", "Track Duration" and "Release Date".

\begin{table}[h]
	\renewcommand{\arraystretch}{1.5}
	\caption{Table of Integrated Schema Attributes}\label{tab2}
	\centering
	\begin{tabular}{p{4cm}p{3cm}p{6cm}}
		\toprule
		\textbf{Attribute Name} &  \textbf{ Datatype}&\textbf{Datasets in which  attribute found}\\
		\hline
		\hline
		Track&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Artist&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Album&string&Spotify  Musicality, Spotify  Streaming  Statistics, Openmusic\\
		Youtube Views&decimal&Spotify  Streaming  Statistics, openmusic\\
		YouTube Likes&decimal&Spotify  Streaming  Statistics\\
		Release Date&datetime&Spotify  Streaming  Statistics, openmusic\\
		Danceability&decimal&Spotify  Streaming  Statistics\\
		Energy&decimal&Spotify  Streaming  Statistics\\
		Key&decimal&Spotify  Streaming  Statistics\\
		Loudness&decimal&Spotify  Streaming  Statistics\\
		Speechiness&decimal&Spotify  Streaming  Statistics\\
		Acousticness&decimal&Spotify  Streaming  Statistics\\
		Instrumentalness&decimal&Spotify  Streaming  Statistics\\
		Liveness&decimal&Spotify  Streaming  Statistics\\
		Valence&decimal&Spotify  Streaming  Statistics\\
		Tempo&decimal&Spotify  Streaming  Statistics\\
		TrackPlaybackClean&string&openmusic\\
		TrackPlaybackExplicit&string&openmusic\\
		\hline
		\hline
	\end{tabular}
\end{table}

\subsubsection{Tools and Challenges}
Tools used include Altova MapForce. Challenges encountered during schema mapping included aligning attributes and handling missing data.

\subsubsection{Conversion to Target Schema}
Datasets were converted to the target schema resulting in XML files.

\section{Identity Resolution}
\subsection{Initiate Gold Standard and Challenges}

At the start, we opted for a simple similarity metric---Edit-Distance (\textit{Levenshtein Distance}) using the single key \texttt{Track}---to build the gold standard for the identity resolution phase. Levenshtein Distance is a widely used general-purpose string similarity metric, particularly effective for handling minor typographical errors, such as added, deleted, or substituted characters. For example, it can detect similarities between \textit{"Someone Like You - (Adele Live)"} and \textit{"Someone Like You - Adele"} (similarity 77\%). However, it struggles with reordered components within strings, such as \textit{"Love Song Taylor"} vs. \textit{"Taylor - Love Song"} (similarity 17\%), which can occur in music track titles.

This approach revealed limitations in coverage, as many tracks in the dataset shared the same name but differed in attributes such as \texttt{Album Name}, \texttt{Year}, and \texttt{Artist}, making them distinct. This issue arose from the dataset itself, not from the metrics, as using only the \texttt{Track} key did not capture the necessary information to distinguish between these cases. Additionally, GPT-4o-mini was used to validate 50 randomly selected cases from 1,000, and its results often diverged from manual labeling, further emphasizing the need for more comprehensive matching criteria. Details of this validation process are discussed in the next section.

The gold standard was constructed by extracting and comparing data from \textit{Apple Music}, the \textit{Million Song Dataset}, and \textit{Open Music DB}. Python tools were used to parse XML files and retrieve key attributes such as \texttt{Track} and \texttt{ID}, while random sampling ensured the datasets remained manageable.

While Levenshtein Distance was chosen for its simplicity and typo tolerance, its limited coverage required incorporating additional attributes like \texttt{Artist} for better accuracy. Manual validation helped address mismatches caused by identical track names with different metadata.

Efficiency was improved using \texttt{Dask} for parallel processing and blocking techniques to reduce unnecessary comparisons. However, the resulting gold standard, though sufficient as a baseline, lacked accuracy and required significant refinements to enhance reliability.

\subsection{Gold Standard Improvement}

To enhance the reliability and efficiency of the gold standard, we implemented several improvements that addressed key limitations of the initial approach. These improvements focused on refining similarity matching, reducing computational overhead, and leveraging advanced validation techniques.

\subsubsection*{Similarity Matching Enhancement}
We enhanced the similarity matching process by concatenating \textit{Track Name}, \textit{Artist Name}, and \textit{Year} into a single string. Similarity scores were calculated using normalized Levenshtein metrics, which allowed us to categorize the results into:
\begin{itemize}
    \item \textbf{Matched Cases}: Similarity scores in the range [0.75, 1].
    \item \textbf{Corner Cases}: Similarity scores in the range (0.65, 0.75).
    \item \textbf{Non-Matched Cases}: Similarity scores in the range [0, 0.65].
\end{itemize}

This approach improved differentiation between tracks with similar names but differing metadata, such as different release years or artists.
\subsubsection*{Integration of GPT-4o-mini for Validation}
We incorporated GPT-4o-mini to validate ambiguous and borderline cases. The model processed 1,173 rows from the \textit{OpenDB and Apple Dataset} and 1,510 rows from the \textit{OpenDB and Million Dataset}. By using contextual prompts containing attributes such as \textit{Track Name}, \textit{Artist Name}, and \textit{Year}, GPT-4o-mini resolved complex cases, such as distinguishing live and studio versions of the same track. The example prompt of Apple Dataset and OpenDB Dataset below illustrates the validation process:	
\begin{tcolorbox}[colback=gray!5!white, colframe=black!75!white, title=Prompt]
	You are a music data expert. Compare these music tracks to determine if they represent the same real-world song. \\
	
	\textbf{Data to compare:} \\
	- \texttt{apple\_Artist} vs \texttt{opendb\_Artist} \\
	- \texttt{apple\_Track} vs \texttt{opendb\_Track} \\
	- \texttt{apple\_Album} vs \texttt{opendb\_Album} \\
	- \texttt{apple\_Year} vs \texttt{opendb\_Year} \\
	- \texttt{apple\_Duration} vs \texttt{opendb\_Duration} \\
	
	\textbf{Rules:} \\
	1. Names should be compared case-insensitively, allowing for semantic similarity rather than exact matches. \\
	2. Duration difference should be within ±3000ms. \\
	3. Year difference should be within ±1 year. \\
	4. All attributes must match within thresholds. \\
	
	\textbf{Output:} Return \texttt{1} if the tracks are the same song, or \texttt{0} if they are different.
	\end{tcolorbox}
	
		
\subsubsection*{Stratified Sampling and Manual Review}
We performed stratified sampling using Python's NumPy, ensuring a balanced dataset with a ratio of \textbf{50:30:20} for Non-Matched, Corner, and Matched cases. Additionally, manual review was conducted on similarity computations and GPT evaluations to resolve ambiguities and finalize labeling. This step ensured a robust and high-quality gold standard.

\subsubsection*{Observations and Results}
These refinements resulted in significant improvements:
\begin{itemize}
    \item Reduced computational overhead through blocking and Bloom Filtering.
    \item Improved match accuracy by incorporating additional attributes such as \textit{Artist Name} and \textit{Year}.
    \item Enhanced validation through GPT-4o-mini for resolving corner cases.
\end{itemize}
\textbf{Blocking and Bloom Filtering} were grouped into logical blocks based on the first two characters of the concatenated string to restrict comparisons to records within the same block, significantly reducing the number of pairwise evaluations; additionally, a Bloom filter was applied within each block to efficiently pre-filter records, ensuring that only likely matches were compared, which optimized the process by reducing pairwise comparisons by approximately 70% while maintaining a low false positive rate of 0.1%.

The refined gold standard is robust and ready for the identity resolution phase, with further confirmation expected after data fusion.

\subsection{Matching Strategies}
\subsubsection{Blocking Methods}

To reduce unnecessary comparisons during identity resolution (IR), we generated blocking keys based on track names. The blocking key was derived from the bigrams of the first three tokens of each track name. We then experimented with two blocking methods: \textbf{Standard Blocking} and the \textbf{Sorted Neighborhood Method}, both using these track name-based blocking keys.

In the case of running IR for the apple + opendb datasets, the maximum number of entities sharing the same hashed blocking key was 9.696 (for the blocking key "CH"), while the minimum was 1 (for the blocking key "LAINTH"). As for the million + opendb datasets, the maximum number of entities sharing the same hashed blocking key was 67.404 (for the blocking key "RE"), while the minimum was 1.560 (for the blocking key "BL"). When using the Sorted Neighborhood Method, the window size would need to be greater than 9.696 and 67.404 respectively to ensure that no matches were missed. However, such a large window size would significantly increase resource consumption by comparing many irrelevant records.

Given this inefficiency, for both of our entity matching comparisons, we ultimately decided to adopt \textbf{Standard Blocking}, which efficiently grouped entities based on their blocking keys without requiring extensive resource expenditure or risking lost matches.
\subsubsection{Similarity Metrics}
As mentioned above, in the entity matching for apple + opendb, we use \textbf{track name, artist name, album name, }and \textbf{album year} as the basis for determining entity matching. As for entity matching for million + opendb, since there is no album name attribute in million dataset, we only use \textbf{track name, artist name}, and \textbf{album year} as the basis. For each attribute with a data type of string, we tested various metrics, including edit-based (Levenshtein, Jaro, Jaro-Winkler), token-based (Jaccard), and phonetic (Soundex). For numeric attribute, we use absolute difference of 2 years.

\subsection{Evaluation}
\subsubsection{Metrics and Analysis}

After testing over 50+ combinations of comparators for entity matching between apple + opendb datasets and million + opendb datasets, the metrics shown in Table~\ref{tab:comparators} were found to be the most suitable for their respective attributes.

% Adjust padding
\setlength{\tabcolsep}{4pt} % Horizontal padding
\renewcommand{\arraystretch}{1.5} % Vertical padding
\begin{table}[ht]
	\centering
	\caption{Comparators Used for Entity Matching Across Datasets}
	\label{tab:comparators}
	\begin{tabular}{p{2.5cm}p{2.5cm}p{6cm}}
	\hline
	\textbf{Dataset}      & \textbf{Attribute (Comparator)}            & \textbf{Reason for Effectiveness}                                                                                      \\ \hline
	apple+opendb; million+opendb & \raggedright Track Name (Jaccard)              & Track names often include variations like additional descriptors, e.g., single, feat.                                  \\ \hline
	apple+opendb                  & \raggedright Artist Name (Jaro-Winkler)                  & Artist names are short and structured. Most typos occur in the last name rather than the first name.                    \\ \hline
	million+opendb                & \raggedright Artist Name (Equal)                          & Many entities in million + opendb have the same track name but different artist names. Within those entities, the artist names are quite similar in some cases; therefore, the equal comparator is needed to clearly distinguish ambiguous entities. \\ \hline
	apple+opendb                  & \raggedright Album Name (Jaccard)              & Most of the singles use the track name as their album name, so the same pattern applies here.                           \\ \hline
	apple+opendb; million+opendb & \raggedright Album Year (Absolute Difference ±2)       & Accounts for real-world scenarios like re-releases and recording/release year discrepancies.                            \\ \hline
	\end{tabular}
	\end{table}

	We organized some of our IR tests in Table~\ref{tab:benchmark}. Those combination we finally chose are highlighted in yellow. In the results for apple + opendb, we intuitively selected the one that demonstrated the best performance across Precision, Recall, and F1-score. Upon closer inspection of the correspondences, the matches were indeed highly accurate, confirming our choice.

	However, in the results for million + opendb, after reviewing the correspondences, we decided to select the option that did not achieve the best performance metrics. This decision was based on the presence of more ambiguous data in this comparison, which, combined with an insufficient golden standard to accurately use the album year to distinguish different entities, resulted in false positives within the correspondences. Under these circumstances, the selected option demonstrated the most balanced precision and recall, minimizing false positives while still capturing the majority of true matches. This balance made it the optimal choice for this dataset.

\begin{table}[]
	\small
	\caption{Entity Matching Benchmark Table}
	\label{tab:benchmark}
	\begin{threeparttable}
	\begin{tabular}{lp{4.5cm}p{2cm}ccccc}
		\hline
		\textbf{Datasets}      & \textbf{Matching Rule}                                          & \textbf{B}                          & \textbf{P} & \textbf{R} & \textbf{F1}  & \textbf{\# Corr}  \\\hline
		apple+opendb         & \raggedright Track (J*), Artist (JW*), \\ Album (J), Album Year (2Y*) & \raggedright SNB Track (20)              & 0,99              & 0,66          & 0,79       & 85               \\
		apple+opendb         & \raggedright Track (J), Artist (JW), \\ Album (J), Album Year (2Y) & \raggedright SNB Track (60)              & 0,99             & 0,82          & 0,89       & 105              \\
		\rowcolor[HTML]{FFFFCC} 
		apple+opendb         & \raggedright Track (J), Artist (JW), \\ Album (J), Album Year (2Y) & \raggedright Standard Track                            & 0,99             & 0,93          & 0,96       & 120              \\
		apple+opendb         & \raggedright Track (L), Artist (JW), \\ Album (L*), Album Year (2Y) & \raggedright Standard Track                            & 0,99             & 0,92           & 0,9544       & 118              \\
		apple+opendb         & \raggedright Track (S*), Artist (JW),\\  Album (S), Album Year (2Y) & \raggedright Standard Track                            & 0,97             & 0,95          & 0,96       & 170              \\
		apple+opendb         & \raggedright Track (J), Artist (J),\\  Album (J)             & \raggedright Standard Track                            & 0,87             & 0,94          & 0,91       & 291              \\ \hline\hline
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright SNB Track (20)              & 0,94             & 0,72         & 0,82       & 684              \\
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright SNB Track (60)              & 0,92             & 0,85         & 0,88       & 835              \\
		\rowcolor[HTML]{FFFFCC} 
		million+opendb       & \raggedright Track (J), Artist (Equal), \\ Album Year (2Y)           & \raggedright Standard Track                            & 0,92             & 0,94         & 0,93      & 942              \\
		million+opendb       & \raggedright Track (J), Artist (JW), \\ Album Year (2Y)     & \raggedright Standard Track                            & 0,91             & 0,99         & 0,95       & 2.078            \\ 
		million+opendb       & \raggedright Track (S), Artist (JW), \\ Album Year (2Y)     & \raggedright Standard Track                            & 0,88             & 0,94         & 0,91       & 1.326            \\
		million+opendb       & \raggedright Track (J), Artist (Equal)                                & \raggedright Standard Track                            & 0,88             & 0,94         & 0,91      & 1.082            \\ \hline\hline
	\end{tabular}
	\begin{tablenotes}
		\footnotesize
		\item \textbf{*Note:} J: Jaccard; JW: Jaro-Winkler; 2Y: Absolute Difference ±2; S: Soundex; L: Levenshtein
		\end{tablenotes}
	\end{threeparttable}
\end{table}

Although the data in the table suggests that the results of Entity Matching are reasonably satisfactory, a closer examination of the million + opendb correspondences reveals that the differentiation of entities based on the album year is insufficient. This limitation is also reflected in the results of our Group Size analysis.
\setlength{\tabcolsep}{4pt} % Horizontal padding
\begin{table}[ht]
	\centering
	\caption{Group Size Analysis}
	\label{tab:group_size}
	\begin{tabular}{lccccc}
	\hline
	\rowcolor[HTML]{D9D9D9} 
	\textbf{Group Size} & \textbf{2} & \textbf{3} & \textbf{4-7} & \textbf{8-13} & \textbf{14} \\ \hline
	\textbf{Frequency}  & 650        & 104        & 45           & 4            & 0           \\ \hline
	\textbf{Distribution} & 81\%       & 13\%       & 6\%          & 0\%           & 0\%         \\ \hline
	\end{tabular}
	\end{table}
	

\subsubsection{Error That Remain}

Our IR results for album years continue to show inconsistencies, with mismatched or slightly inaccurate years persisting even when using a tolerance of ±2 years. These discrepancies are particularly evident when dealing with re-releases, remasters, or cases where album metadata varies significantly across datasets.

We think that the root cause of this issue lies in the golden standard we employed, which was derived by concatenating track name, artist name, and album year into a single string and performing an initial comparison using the edit-based Levenshtein metric. Since album years were not accurately distinguished in this process, it led to the observed inconsistencies. This misalignment in album years will negatively impact attribute consistency during the Data Fusion phase. If this issue remains unresolved, the resulting fused data will likely inherit these inconsistencies, compromising its overall quality and trustworthiness.
\section{Data Fusion}
\subsection{Fusion Rules}
\subsubsection{Conflict Resolution Strategies}
In the data fusion process, conflict resolution strategies are essential to derive the most accurate and reliable representation of records when combining multiple datasets. Our conflict resolution strategies include
using reliable datasets with sufficient attribute intersection for fusion, as well as strong identity resolution results that maintain quality of data. We decided to use all of our datasets (apple, million, opendb) as
they fulfil our prerequisites with IR results as shown above. We had assigned a provenance score to each dataset in order to reflect its reliability and accuracy. 'apple' dataset was given the highest score (3.0), followed by 'million' (2.0), and then 'opendb' (1.0).
This scoring system allowed the fusion process to favor attributes from the more reliable datasets when resolving conflicts. 

\subsubsection{Specific Fusion Rules}
Our team have included a specific fusion rules for resolving conflicts in key attributes to ensure that the fused dataset accurately reflects the most reliable, comprehensive, and meaningful information. These rules are tailored to the nature of each attribute and the type of conflicts typically encountered.

For the 'Artist' attribute, the Shortest String Strategy was implemented through ArtistFuserShortestString(). This approach resolves discrepancies by selecting the shortest, yet most concise, representation of an artist's name, eliminating unnecessary or redundant descriptors while preserving clarity (e.g., choosing "Adele" over "Adele Laurie Blue Adkins").
Similarly, for textual fields like 'Album' and 'Track', the Shortest String Strategy was applied through AlbumFuserShortestString() and TitleFuserShortestString() respectively. This strategy ensured that album and track names were concise and free of extraneous details, such as track version descriptors, while retaining essential information. For example, a track title such as "Imagine" was favored over "Imagine - 2011 Remastered Version" to maintain simplicity and clarity without losing its core identity.

For Album Year, the Most Recent Value Strategy, applied through AlbumYearFuserMostRecent(), prioritized the latest recorded year from the datasets. This rule is particularly effective for resolving inconsistencies stemming from re-releases or updates in metadata, ensuring that the fused dataset reflects the most up-to-date temporal information. Similarly, the Most Recent Value Strategy was utilized for the Duration and Track Explicitness attributes through DurationFuserMostRecent() and ExplicitnessMostRecent() respectively. These rules leveraged the recency of data sources to ensure accuracy for numeric values like track duration and categorical data like explicitness labels, which are more likely to evolve over time.


\subsection{Fused Data Output}
\subsubsection{Post-Fusion Dataset}

The post-fusion dataset showcases significant improvements in data quality, consistency, and usability. Following the data fusion process, the fused dataset comprises a total of 803 unique records, reflecting a successful consolidation of overlapping information from the input datasets. The density of attributes, particularly for fields such as 'Artist', and 'Album', reached an impressive consistency level of 1.00
, ensuring that these attributes are fully populated across all records. Other attributes, such as 'Duration', 'Track' and 'Track_Explicitness', exhibit high consistency levels of 0.88, 0.98 and 0.99 respectively, indicating minimal discrepancies. In contrast, attributes like 'Album Year', while improved, show a moderate consistency level of 0.56, highlighting areas where additional data enrichment or refinement could further enhance accuracy.
The table below shows a few examples of our fused data records.

We have also decided to compare our fused dataset against the gold standard we have created to demonstrate the effectiveness of the applied fusion strategies. The gold standard, which serves as a benchmark for measuring the correctness of the fused data, comprises 20 meticulously curated records as shown below

Overall, the fused dataset provides a robust, high-density representation of the musical records, enabling reliable downstream analysis and applications. The final accuracy of 0.84 against the gold standard underscores the success of the fusion process while highlighting areas for potential improvement in future iterations.

\subsection{Quality Evaluation}
We conducted extensive testing with various combinations of fusion methods. Table \ref{tab:fusion-strategies} shows four representative strategies that combine different fusion methods, including LongestString(), FavourSource(), ShortestString(), and MostRecent(). Strategy 4 demonstrated the highest accuracy at 84\% and was therefore selected as our optimized fusion strategy.

\begin{table}[h!]
	\setlength{\tabcolsep}{4pt}
	\small
	\caption{Data Fusion Strategy Comparison}
	\label{tab:fusion-strategies}
	\begin{threeparttable}
	\begin{tabular}{p{2.5cm}p{7.5cm}c}
	\hline
	\textbf{Fusion Strategy} & \textbf{Method} & \textbf{Accuracy} \\ \hline
	Strategy 1 & \raggedright ArtistFuserLongestString(), \newline
	AlbumYearFuserVoting(), \newline
	AlbumFuserLongestString(), \newline
	DurationFuserFavourSource(), \newline
	TitleFuserLongestString(), \newline
	ExplicitnessFuserFavourSource() & 80\% \\
	\hline
	Strategy 2 & \raggedright ArtistFuserFavourSource(), \newline
	AlbumYearFuserFavourSource(), \newline
	AlbumFuserFavourSource(), \newline
	DurationFuserFavourSource(), \newline
	TitleFuserFavourSource(), \newline
	ExplicitnessFuserFavourSource() & 80\% \\
	\hline
	Strategy 3 & \raggedright ArtistFuserShortestString(), \newline
	AlbumYearFuserVoting(), \newline
	AlbumFuserShortestString(), \newline
	DurationFuserFavourSource(), \newline
	TitleFuserShortestString(), \newline
	ExplicitnessFuserFavourSource() & 82\% \\
	\hline
	\rowcolor[HTML]{FFFFCC}
	Strategy 4 \newline (Optimized) & \raggedright ArtistFuserShortestString(), \newline
	AlbumYearFuserMostRecent(), \newline
	AlbumFuserShortestString(), \newline
	DurationFuserMostRecent(), \newline
	TitleFuserShortestString(), \newline
	ExplicitnessFuserMostRecent() & 84\% \\ \hline
	\end{tabular}
	\end{threeparttable}
	\end{table}

	To evaluate our optimized fusion strategy's effectiveness, we analyzed its performance across different attributes (Table \ref{table:optimized_fusion_rules}). While most attributes achieved high accuracy, Album Name information showed notably low accuracy (50\%) due to the frequent occurrence of the same track appearing in different album releases (e.g., singles, original albums, compilation albums). This common practice in the music industry creates challenges in determining the canonical album name for a track, leading lower accuracy and consistency scores for Album Name (50\%, 81\%) and AlbumYear (75\%, 56\%) attributes.

	\begin{table}[h!]
		\centering
		\caption{Optimized Fusion Strategy Performance}
		\label{table:optimized_fusion_rules}
		\begin{tabular}{llccc}
		\toprule
		\textbf{Attribute} & \textbf{Method} & \textbf{Accuracy} & \textbf{Consistency} & \textbf{Density} \\ 
		\midrule
		Artist             & ArtistFuserShortestString()           & 95\%              & 100\%                & 100\%            \\ 
		AlbumYear        & AlbumYearFuserMostRecent()           & 75\%              & 56\%                 & 100\%            \\ 
		Album              & AlbumFuserShortestString()           & 50\%              & 81\%                 & 100\%            \\ 
		Duration           & DurationFuserMostRecent()            & 85\%              & 88\%                 & 100\%            \\ 
		Track              & TitleFuserShortestString()           & 85\%              & 98\%                 & 100\%            \\ 
		TrackExplicitness & ExplicitnessFuserMostRecent()        & 90\%              & 99\%                 & 100\%            \\ 
		\bottomrule
		\end{tabular}
		\end{table}
\section{Conclusion and Future Work}
\subsection{Limitations of the Project}
Discussed limitations such as incomplete data and computational constraints.

\subsection{Recommendations for Future Improvements}
Recommendations included using advanced ML models for identity resolution.

\bibliographystyle{plainurl}
\bibliography{document.bib} 
\end{document}
